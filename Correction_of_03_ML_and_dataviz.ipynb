{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Correction_of_03_ML_and_dataviz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annemariet/tutorials/blob/master/Correction_of_03_ML_and_dataviz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXwvUy0E9vy_"
      },
      "source": [
        "# ML4Dataviz / Dataviz4ML, let's practice!\n",
        "\n",
        "This practical is organized in two parts. **The first is \"ML4Dataviz\"**: we are going to explore the MNIST dataset using high-dimensional projections. The goal is that you get a feeling of how complex datasets can be visualized through classical linear and non-linear projections.\n",
        "\n",
        "- You are expected to write mostly the `dataviz` part of the code (ie `matplotlib`, `seaborn`, etc)\n",
        "- You are expected to play with the parameters of the projection functions and explain how they impact performance & display.\n",
        "\n",
        "**The second part is \"Dataviz4ML\"**: We will play with the Fashion-MNIST dataset. A simple feed-forward neural network is implemented. Knowing `keras` is not a prerequisite. The goal in this section is to work you through some examples of how visualization will help you understand & debug your models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dNVmq1rneej"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn import random_projection, decomposition, manifold\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import offsetbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSkOkKfGNXzR"
      },
      "source": [
        "# 1. Visualizing MNIST\n",
        "\n",
        "We use the `scikit-learn` library to download the data and matplotlib for preliminary basic visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKIFi1slt1Km"
      },
      "source": [
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
        "X = X / 255.\n",
        "y = y.astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAyfq8_4OZyJ"
      },
      "source": [
        "For simplicity we will only work on the first 1000 samples. Not that the usual split uses the first 60k samples for training and the rest for test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbesdVprpwUh"
      },
      "source": [
        "n_vis_samples = 1000\n",
        "w, h = 28, 28\n",
        "X_vis = X[:n_vis_samples]\n",
        "y_vis = y[:n_vis_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZWsx7VgOm8U"
      },
      "source": [
        "It's a good idea to have a look at the raw data to begin with. In the specific case of images we can use a the matplotlib function `imshow` and choose the colormap we prefer. \n",
        "\n",
        "Check the difference between `gray` and `gray_r` colormaps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QfW5FJNrIMP"
      },
      "source": [
        "ax = plt.subplot(111)\n",
        "ax.imshow(X_vis[0,:].reshape(w, h), cmap=plt.cm.gray_r)\n",
        "ax.set_title(y_vis[0])\n",
        "ax.set_axis_off();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ZwNjX8ZudA"
      },
      "source": [
        "We're going to plot images often, so it might help to have a function for this. Define a function plot_image using the following prototype:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkGN_84pZrjA"
      },
      "source": [
        "def plot_image(img, reshape_size=None, cmap=\"binary\", ax=None, title=None):\n",
        "  if ax is None:\n",
        "    ax = plt.subplot(111)\n",
        "  if reshape_size is not None:\n",
        "    img = img.reshape(*reshape_size)\n",
        "  ax.imshow(img, cmap=cmap)\n",
        "  if title is not None:\n",
        "    ax.set_title(title)\n",
        "  ax.set_axis_off();\n",
        "  return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ7Q7f_K-MBh"
      },
      "source": [
        "plot_image(X_vis[0, :], reshape_size=(w, h), cmap=plt.cm.gray_r, title=y_vis[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoYpSM-9bRE6"
      },
      "source": [
        "Now you can use this function to plot a bunch of images from the dataset in a grid and have a feeling of what MNIST digits look like. you can play around with the parameters such as colormaps, grid size, etc..\n",
        "\n",
        "You can use the [`plt.subplots`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.subplots.html) from matplotlib to draw several axes on the same figure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUq-OWj9q3eO"
      },
      "source": [
        "n_img_per_row = 20\n",
        "fig, axes = plt.subplots(n_img_per_row // 4, n_img_per_row, figsize=(25,8))\n",
        "for i in range(n_img_per_row // 4):\n",
        "  for j in range(n_img_per_row):\n",
        "    im_index = n_img_per_row * i + j\n",
        "    plot_image(X_vis[im_index], \n",
        "               reshape_size=(w, h),\n",
        "               #cmap=plt.cm.gray_r,\n",
        "               ax=axes[i, j],\n",
        "               title=y_vis[im_index])\n",
        "plt.suptitle(\"MNIST\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DNORvKqNgMY"
      },
      "source": [
        "## High-dimensional projections to 2D\n",
        "\n",
        "In this exercise we are going to use classical methods to visualize MNIST data in 2D.\n",
        "\n",
        "The following sample code is adapted from the [scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vmEe41yrorm"
      },
      "source": [
        "What is the original dimension of MNIST features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs_UY4mTruKr"
      },
      "source": [
        "X_vis.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfQ1LE7zBhdF"
      },
      "source": [
        "We define a helper function to plot the digits in `y` at positions in `X2d`. The goal of the exercise is to compute different 2D positions for the same original image vectors and get an intuition about how useful they are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-j6v_UqANx"
      },
      "source": [
        "def plot_digits_embedding(X2d, y, title=None, remove_ticks=True):\n",
        "  \"\"\"\n",
        "  Plot a 2D points at positions `X2d` using text labels from `y`.\n",
        "  The data is automatically centered and rescaled to [0,1].\n",
        "  Ticks are removed by default since the axes usually have no meaning (except for PCA).\n",
        "  \"\"\"\n",
        "  x_min, x_max = np.min(X2d, 0), np.max(X2d, 0)\n",
        "  X = (X2d - x_min) / (x_max - x_min)\n",
        "\n",
        "  plt.figure(figsize=(20,10))\n",
        "  ax = plt.subplot(111)\n",
        "  for i in range(X.shape[0]):\n",
        "    plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
        "                color=plt.cm.tab10(y[i]),\n",
        "                fontdict={'weight': 'bold', 'size': 9})\n",
        "\n",
        "  if remove_ticks:\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "  if title is not None:\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Lccq4YsukO"
      },
      "source": [
        "We start with a random projection. Even though projecting randomly in 2 dimensions is a rather bad idea, when the original dimension is huge, a random projection to a smaller, still high, dimension might give a nice speedup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCU772sKqj2p"
      },
      "source": [
        "print(\"Computing random projection\")\n",
        "t0 = time()\n",
        "rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\n",
        "X_projected = rp.fit_transform(X_vis)\n",
        "plot_digits_embedding(X_projected, y_vis, \"Random Projection of the digits (time %.2fs)\" %\n",
        "               (time() - t0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tJIatpFCKZy"
      },
      "source": [
        "### PCA\n",
        "\n",
        "PCA is the go-to method for dimensionality reduction (not only for visualisation). Play around with it to get a feel of how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu7UmO0MtUPC"
      },
      "source": [
        "You can use scikit-learn's PCA. What is it using under the hood?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT18tTHEqrCx"
      },
      "source": [
        "print(\"Computing PCA projection\")\n",
        "t0 = time()\n",
        "pca = decomposition.PCA(n_components=2, svd_solver=\"arpack\")\n",
        "X_pca = pca.fit_transform(X_vis)\n",
        "plot_digits_embedding(X_pca, y_vis, \n",
        "               \"Principal Components projection of the digits (time %.2fs)\" %\n",
        "               (time() - t0),\n",
        "               remove_ticks=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcaztxsdW4Bb"
      },
      "source": [
        "*Note that zeros are mostly on the X-axis (around 0.8, 06) and ones are mostly on the second (0, 0.8)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzENxQGLwrPY"
      },
      "source": [
        "Compare the different `svd_solver` efficiency. `%timeit` is a notebook magic that allows you to compute the time taken by a line of code. Which method is best if you use the full dataset, vs only a few? (a few: arpack or randomized, full: randomized)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W2RpsG0vr53"
      },
      "source": [
        "%timeit decomposition.PCA(n_components=2, svd_solver=\"full\").fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrgAf4wrvyIb"
      },
      "source": [
        "%timeit decomposition.PCA(n_components=2, svd_solver=\"arpack\").fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzuFwzlHwIml"
      },
      "source": [
        "%timeit decomposition.PCA(n_components=2, svd_solver=\"randomized\").fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-YMCnIdNUPf"
      },
      "source": [
        "Since PCA is a linear decomposition, each component is itself an image. Let's have a look at these \"eigenimages\". They are stored in the `components_` attribute of the sklearn `PCA` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ur8rd0uLwcu"
      },
      "source": [
        "ax = plt.subplot(121)\n",
        "plot_image(pca.components_[0,:].reshape(w, h), ax=ax, title=\"PCA first axis\")\n",
        "ax=plt.subplot(122)\n",
        "plot_image(pca.components_[1,:].reshape(w, h), ax=ax, title=\"PCA second axis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE1VJ7-gyGBQ"
      },
      "source": [
        "Do you think the projection is good? Can you quantify how good it is? How can you explain this? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onxlDgIXOBpD"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8RlvhaWyakG"
      },
      "source": [
        "_PCA_ is limited to linear transformations of the data. There is little chance that you could find only 2 directions to span images which live in a high-dimensional space. \n",
        "Can non-linear methods give us better results here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQDbtmOuCFnu"
      },
      "source": [
        "### Multidimensional Scaling (MDS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKvdbaNxvxo3"
      },
      "source": [
        "print(\"Computing MDS embedding\")\n",
        "clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\n",
        "t0 = time()\n",
        "X_mds = clf.fit_transform(X_vis)\n",
        "print(\"Done. Stress: %f\" % clf.stress_)\n",
        "plot_digits_embedding(X_mds[:n_vis_samples], y_vis, \n",
        "               \"MDS embedding of the digits (time %.2fs)\" %\n",
        "               (time() - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znfGSMj2XMKp"
      },
      "source": [
        "*Pas trÃ¨s convaincant, bien qu'on voie quelques chiffres se regrouper.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBPI-3l4BLPn"
      },
      "source": [
        "### t-SNE\n",
        "\n",
        "Play with the various parameters and observe how time/cluster change. [Scikit-learn documentation](https://scikit-learn.org/stable/modules/manifold.html#t-sne) is a good starting point if you want to know more.\n",
        "\n",
        "- perplexity (`perplexity`)\n",
        "- early exaggeration factor (`early_exaggeration`)\n",
        "- learning rate (`learning_rate`)\n",
        "- maximum number of iterations (`n_iter`)\n",
        "- angle (`angle`, not used in the exact method)\n",
        "\n",
        "Which parameters have more impact?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1C-GDVjvAv0"
      },
      "source": [
        "tsne = manifold.TSNE(n_components=2, init='random', random_state=42, perplexity=30)\n",
        "t0 = time()\n",
        "\n",
        "X_tsne = tsne.fit_transform(X[:5000]) \n",
        "\n",
        "plot_digits_embedding(X_tsne[:n_vis_samples], y_vis, \n",
        "               \"t-SNE embedding of the digits (time %.2fs)\" %\n",
        "               (time() - t0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t48HRvfjSLtw"
      },
      "source": [
        "### UMAP\n",
        "\n",
        "UMAP is not yet integrated into scikit-learn but is available as a standalone library that follows the sklearn API. You can have a look at [their doc](https://umap-learn.readthedocs.io/en/latest/parameters.html) to get a better understanding of the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsfjsk3XUB_Q"
      },
      "source": [
        "#!pip install umap-learn # run this if you run into an \"ModuleNotFoundError: No module named 'umap'\" error below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em83Z-q7Q0gw"
      },
      "source": [
        "import umap\n",
        "t0 = time()\n",
        "um = umap.UMAP(n_neighbors=20, min_dist=0.1, metric=\"euclidean\")\n",
        "#um.fit_transform(X_vis)  # faster, but fitting with more samples below give a cleaner output\n",
        "X_umap = um.fit_transform(X[:5000])\n",
        "t1 = time()\n",
        "fig_umap = plot_digits_embedding(X_umap[:n_vis_samples], y_vis, \n",
        "               \"UMAP embedding of the digits (time %.2fs)\" %\n",
        "               (t1 - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAlgIa60WRWd"
      },
      "source": [
        "In this example, UMAP yields a much cleaner plot, and is faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIu6EulW4mOl"
      },
      "source": [
        "If time permits, you can try combining the methods above: for instance, use PCA to reduce the dimensionality to accelerate either MDS or tSNE. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzLfTNtp44EM"
      },
      "source": [
        "#TODO_OPTIONAL\n",
        "t0 = time()\n",
        "# Use PCA to get only 50 dimensions, then UMAP\n",
        "pca = decomposition.PCA(n_components=10, svd_solver=\"auto\")\n",
        "tsne = manifold.TSNE(n_components=2, init='random', random_state=42, perplexity=30)\n",
        "\n",
        "X_pca = pca.fit_transform(X[:5000])\n",
        "X_tsne = tsne.fit_transform(X_pca) \n",
        "t1 = time()\n",
        "fig_tsne_pca = plot_digits_embedding(X_tsne[:n_vis_samples], y_vis, \n",
        "               \"PCA-TSNE embedding of the digits (time %.2fs)\" %\n",
        "               (t1 - t0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogdpxyso2dmE"
      },
      "source": [
        "*The result is about the same as with TSNE, but twice faster. Note that 784 is not that big and the speed-up may vary depending on the methods you use, especially as it also depends on the number of points.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwB_56JVQIdq"
      },
      "source": [
        "## The same plots with Bokeh\n",
        "\n",
        "We are going to improve the visualization of `X_projected`, `X_pca`, `X_mds` and `X_tsne` by using `bokeh`.\n",
        "\n",
        "\n",
        "If the Bokeh interactive plots don't show as expected, you might have to revert to a previous version of Bokeh (run `!pip install bokeh==1.3.4`) and restart your runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eECWoWCAvuzS"
      },
      "source": [
        "import bokeh\n",
        "from bokeh.models.sources import ColumnDataSource\n",
        "from bokeh.models.tools import HoverTool\n",
        "import matplotlib as mpl\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "output_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priJq0pr7HpB"
      },
      "source": [
        "You are provided with two helper functions to transform images to bokeh-compatible formats. Replace the call to `circle` with `image` and `image_rgba` to change the glyphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2RBzrOyzL2"
      },
      "source": [
        "\n",
        "def im_colorize_rgba(arr, label):\n",
        "  color = mpl.cm.tab10(label)\n",
        "  if arr.ndim == 1:\n",
        "    out_img = ((1 - arr[:, np.newaxis]) * np.array(color)).reshape(28,28,4) * 255\n",
        "  elif arr.ndim == 2:\n",
        "    out_img = (arr[:, :, np.newaxis] * np.array(color)) * 255\n",
        "  else:\n",
        "    raise ValueError(\"Image format not handled (yet).\")\n",
        "  out_img = np.flipud(out_img)\n",
        "  return np.squeeze(out_img.astype(np.uint8)).view(np.uint32)\n",
        "\n",
        "def im_reshape_bw(arr):\n",
        "  out_img = (1 - arr.reshape(28,28))*255\n",
        "  out_img = np.flipud(out_img)\n",
        "  return out_img\n",
        "  \n",
        "def plot_interactive_embedding(X, y, X_img, title=None, dscale=3):\n",
        "  \"\"\"\n",
        "  X: 2D projection of the input vectors (floats)\n",
        "  y: label/category indexes (integers)\n",
        "  X_img: original input vectors (images as vectors of dimension 784)\n",
        "  title (optional): plot title\n",
        "  dscale (default=3): control image glyphs, should be adjusted depending on the scale of X\n",
        "  \"\"\"\n",
        "  bokeh_data = ColumnDataSource(\n",
        "          data=dict(\n",
        "              x1 = X[:,0],\n",
        "              x2 = X[:,1],\n",
        "              colors = [\"#%02x%02x%02x\" % (int(r), int(g), int(b)) for r, g, b, _ in \n",
        "                        255*mpl.cm.tab10(y)],\n",
        "              images = [im_reshape_bw(xi) for xi in X_img],\n",
        "              images_rgba = [im_colorize_rgba(xi, yi) for xi, yi in zip(X_img, y)],\n",
        "              labels = [str(yi) for yi in y]\n",
        "          )\n",
        "      )\n",
        "  hover_tsne = HoverTool(tooltips='@labels')\n",
        "  tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
        "  plot_tsne = figure(plot_width=600, plot_height=600, tools=tools_tsne, title=title)\n",
        "\n",
        "  #plot_tsne.circle(\"x1\", \"x2\", size=4, fill_color=\"colors\", line_width=0, source=bokeh_data)\n",
        "  plot_tsne.image_rgba(image=\"images_rgba\", x=\"x1\", y=\"x2\", dw=dscale, dh=dscale, source=bokeh_data)\n",
        "  #plot_tsne.image(image=\"images\", x=\"x1\", y=\"x2\", dw=2, dh=2, source=bokeh_data)\n",
        "\n",
        "  return plot_tsne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRI3BQWxy26p"
      },
      "source": [
        "fig_tsne = plot_interactive_embedding(X_tsne[:n_vis_samples], y_vis, X_vis, \n",
        "               \"t-SNE embedding of the digits\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJbCxY0f2gSi"
      },
      "source": [
        "show(fig_tsne)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws_QR6tvRErV"
      },
      "source": [
        "fig_umap = plot_interactive_embedding(X_umap[:n_vis_samples], y_vis, X_vis, \n",
        "               \"UMAP embedding of the digits\", dscale=0.3)\n",
        "show(fig_umap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLJEo7H47xLK"
      },
      "source": [
        "# 2. Visualizing and debugging neural networks\n",
        "\n",
        "In the next section, we're going to reproduce what you're learnt before in your machine learning courses with a focus on the visualizing part of it. You should come out with handy functions that you can reuse later for your own experiments.\n",
        "\n",
        "We will use Fashion-MNIST because that's more fun to visualize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFf-c82uYsjo"
      },
      "source": [
        "Build an image classifier & visualize layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG2jMW1YJiqK"
      },
      "source": [
        "# adapted from https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBpMnKy9Xf94"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IFDUAn7Xgqe"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPrUnBwAZZxs"
      },
      "source": [
        "X_train_full.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0aiFOZoYruA"
      },
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Eexz4WdcgbM"
      },
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "532vkUdWZel1"
      },
      "source": [
        "plot_image(X_train[0], reshape_size=None, cmap=\"binary\", ax=None, title=class_names[y_train[0]]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNhLg2JgZh64"
      },
      "source": [
        "n_rows = 5\n",
        "n_cols = 20\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        ax = plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plot_image(X_train[index], ax=ax, title=class_names[y_train[index]]);\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq1Fk1i_0EdA"
      },
      "source": [
        "n_vis_samples = 1000\n",
        "X_vis = X_test[:n_vis_samples]\n",
        "y_vis = y_test[:n_vis_samples]\n",
        "X_in = X_vis.reshape(n_vis_samples, 28*28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4l1T9FCz9W3"
      },
      "source": [
        "um = umap.UMAP(n_neighbors=20, min_dist=0.1, metric=\"euclidean\")\n",
        "X_2D = um.fit_transform(X_in)\n",
        "fig_2D_orig = plot_interactive_embedding(X_2D, y_vis, X_vis, \n",
        "               \"UMAP embedding of fashion MNIST from image space\", dscale=0.5)\n",
        "show(fig_2D_orig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rbrZhrsd0u"
      },
      "source": [
        "## A simple fully connected neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-WSj3hbc-pQ"
      },
      "source": [
        "model_fc = keras.models.Sequential()\n",
        "model_fc.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model_fc.add(keras.layers.Dense(50, activation=\"relu\", name=\"fc1\"))\n",
        "model_fc.add(keras.layers.Dense(100, activation=\"relu\", name=\"fc2\"))\n",
        "model_fc.add(keras.layers.Dense(10, activation=\"softmax\", name=\"cls_1\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NBxaVsGdGLK"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UsaqRBQdHnr"
      },
      "source": [
        "keras.utils.plot_model(model_fc, \"my_mnist_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg690CczdKcb"
      },
      "source": [
        "model_fc.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrIVOk4D2GdF"
      },
      "source": [
        "# before fitting\n",
        "layer_to_plot = [\"fc1\", \"fc2\"]\n",
        "features = {}\n",
        "X_in = X_vis.reshape(n_vis_samples, 28*28)\n",
        "for l in model_fc.layers:\n",
        "  X_out = l(X_in)\n",
        "  X_in = X_out\n",
        "  if l.name in layer_to_plot:\n",
        "    features[l.name] = X_out\n",
        "\n",
        "X_2D = um.fit_transform(features[\"fc2\"])\n",
        "fig_2D_fc2_rand = plot_interactive_embedding(X_2D, y_vis, X_vis,\n",
        "               \"UMAP embedding of fashion MNIST from feature space fc2, without training\", dscale=0.5)\n",
        "show(fig_2D_fc2_rand)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buLlTEw-aLSg"
      },
      "source": [
        "*This is not exactly like a random projection, but basically it doesn't look like the structure of the data has changed much.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dVSnu6gdRcy"
      },
      "source": [
        "# training takes about 1 minute\n",
        "history = model_fc.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zyL7RTjC6ZQ"
      },
      "source": [
        "### Looking at the learned weights directly\n",
        "\n",
        "The first layer's weights is a $784\\times h$ matrix, so we can observe a few lines of this matrix as images. Can you see patterns emerge?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpQZTMqdboCK"
      },
      "source": [
        "*It looks like most of the work is done by the first layer -- this may need the network needs longer training time, or better tuning.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PICOXzf68fVn"
      },
      "source": [
        "layer_weights = model_fc.get_weights()\n",
        "len(layer_weights), layer_weights[0].shape, layer_weights[1].shape, layer_weights[2].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FzX2Uw-8dEN"
      },
      "source": [
        "fig, axes = plt.subplots(4, 4)\n",
        "# use global min / max to ensure all weights are shown on the same scale\n",
        "vmin, vmax = layer_weights[0].min(), layer_weights[0].max()\n",
        "\n",
        "for coef, ax in zip(layer_weights[0].T, axes.ravel()):\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
        "               vmax=.5 * vmax)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meXN7VOILgQx"
      },
      "source": [
        "*Even though the images are very noisy, this doesn't look like pure noise, there are indeed faint patterns encoded in the first layer's parameters.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gor7O4HIDWFJ"
      },
      "source": [
        "### Looking at the latent representations\n",
        "\n",
        "Another way to look at the intermediate layers is through their effect as feature transforms.\n",
        "\n",
        "Plot a view of the points as transformed at different levels in the network. What can you observe?\n",
        "\n",
        "_We would expect that categories get easier to separate as we go deeper in the network_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKiDB7wi3NVz"
      },
      "source": [
        "# after fitting\n",
        "layer_to_plot = [\"fc1\", \"fc2\"]\n",
        "features = {}\n",
        "X_in = X_vis.reshape(n_vis_samples, 28*28)\n",
        "for l in model_fc.layers:\n",
        "  X_out = l(X_in)\n",
        "  X_in = X_out\n",
        "  if l.name in layer_to_plot:\n",
        "    features[l.name] = X_out\n",
        "\n",
        "\n",
        "X_2D = um.fit_transform(features[\"fc1\"])\n",
        "fig_2D_fc1 = plot_interactive_embedding(X_2D, y_vis, X_vis, \n",
        "               \"UMAP embedding of fashion MNIST from feature space fc1, _after_ training\", dscale=0.5)\n",
        "\n",
        "X_2D = um.fit_transform(features[\"fc2\"])\n",
        "fig_2D_fc2 = plot_interactive_embedding(X_2D, y_vis, X_vis, \n",
        "               \"UMAP embedding of fashion MNIST from feature space fc2, _after_ training\", dscale=0.5)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cfQ0y602gn3"
      },
      "source": [
        "show(fig_2D_fc1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kugkRKIXriaX"
      },
      "source": [
        "show(fig_2D_fc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DdAO9ttLsGf"
      },
      "source": [
        "### Debugging Learning\n",
        "\n",
        "Plot the learning curve from the history output of the `fit` call. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DCgKLosdR66"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FCWJNgIL69v"
      },
      "source": [
        "_Answer: the learning curve is plateauing, it might be a good idea to lower the learning rate (ReduceLROnPlateau, or use some schedule)._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQMNmIriMLDg"
      },
      "source": [
        "### What do predictions look like?\n",
        "\n",
        "In this part, we're going to look at the output of the classification model itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdWXQdEOdWWd"
      },
      "source": [
        "# Look at a few random predictions\n",
        "n_test = 10\n",
        "X_new = X_test[:n_test]\n",
        "y_proba = model_fc.predict(X_new)\n",
        "y_proba.round(2)\n",
        "y_pred = np.argmax(model_fc.predict(X_new), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-SKMdPGdZ8e"
      },
      "source": [
        "# This could be turned into a function\n",
        "plt.figure(figsize=(20, 3))\n",
        "for index, image in enumerate(X_new):\n",
        "    ax = plt.subplot(1, n_test, index + 1)\n",
        "    title=\"\\n\".join((class_names[y_test[index]], class_names[y_pred[index]]))\n",
        "    plot_image(image, ax=ax, title=title);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De9CpOvpMZqI"
      },
      "source": [
        "#### The distribution of predictions\n",
        "\n",
        "You can use `seaborn`, `bokeh`, or raw `matplotlib` to view these distributions. It might be useful to organise the predictions with dataframes.\n",
        "\n",
        "1. Which classes are predicted the more, the less?\n",
        "2. What is the distribution of the predicted probabilities for each class? Do you see any trend? Does it raise questions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmZoTEOitIFQ"
      },
      "source": [
        "# compute test predictions\n",
        "y_proba_test = model_fc.predict(X_test)\n",
        "y_pred = y_proba_test.argmax(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1cLqQTChDof"
      },
      "source": [
        "ax = plt.subplot(111)\n",
        "ax.hist([y_pred.astype(int), y_test], rwidth=0.7, bins=np.arange(11), label=['predicted', 'true'])\n",
        "ax.set_xticks(np.array(range(10))+0.5)\n",
        "plt.xticks(rotation=75)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "ax.set_xticklabels(class_names);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhS4WcP7dAy0"
      },
      "source": [
        "*Answer 1: Pullover is under-predicted, while Coat is overpredicted.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHe26uOFvdWs"
      },
      "source": [
        "# looking at all the probabilities output vectors\n",
        "# we expect something like 90% close to 0, 10% close to one, maybe more in-between if there is uncertainty in predictions\n",
        "df = pd.DataFrame(data=y_proba_test, columns=class_names)\n",
        "df = df.stack()\n",
        "df.index.names = [\"id\", \"class_name\"]\n",
        "df.name = \"predicted\"\n",
        "df_tidy = df.reset_index(level=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeTDp8z0-T8H"
      },
      "source": [
        "df2 = pd.DataFrame(data=y_test, columns=[\"true\"])\n",
        "df_tidy = df.reset_index(level=1).join(df2)\n",
        "df_tidy.sort_values(by=\"class_name\", inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "279ibGWw9SHk"
      },
      "source": [
        "df_tidy.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YH_Emv30dlnv"
      },
      "source": [
        "df_tidy.groupby(\"class_name\").describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6wFkasNthDY"
      },
      "source": [
        "# adapted from https://seaborn.pydata.org/examples/kde_ridgeplot.html\n",
        "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
        "g = sns.FacetGrid(df_tidy, row=\"class_name\", hue=\"class_name\", aspect=10, height=0.8, palette=pal, sharey=False)\n",
        "\n",
        "# Draw the densities in a few steps\n",
        "#g.map(sns.distplot, \"predicted\", kde=False, hist=True, rug=False, bins=100)\n",
        "g.map(sns.kdeplot, \"predicted\", clip_on=False, shade=True, alpha=1, lw=3) #, bw=.05)\n",
        "g.map(plt.axhline, y=0, lw=5, clip_on=False)\n",
        "def label(x, color, label):\n",
        "    ax = plt.gca()\n",
        "    ax.text(-0.1, .2, label, fontweight=\"bold\", color=color,\n",
        "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
        "\n",
        "g.map(label, \"predicted\")\n",
        "\n",
        "# Set the subplots to overlap\n",
        "g.fig.subplots_adjust(hspace=-.02)\n",
        "g.set_titles(\"\")\n",
        "g.set(yticks=[])\n",
        "g.despine(bottom=True, left=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynL9wX88dgOe"
      },
      "source": [
        "*Answer 2 (trends): Unless you use a different scale on the y-axis, you can't see much differnce between classes, only that the divide is so clear between high and low predictions.\n",
        "Looking with different scales helps: the classification is much easier for some classes (as could be expected by looking at the clusters on the feature projections).*\n",
        "\n",
        "*Next we'll restrict to the max proba for each sample to zoom on the proba of actual predictions.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFTJTzephkNB"
      },
      "source": [
        "y_proba_max = y_proba_test[np.arange(len(y_pred)), y_pred]\n",
        "df = pd.DataFrame(data=np.array([y_proba_max, y_pred, y_test]).T, columns=[\"proba\", \"predicted\", \"true\"])\n",
        "df[[\"predicted\", \"true\"]] = df[[\"predicted\", \"true\"]].astype(int)\n",
        "df[\"class_name\"] = df[\"predicted\"].apply(lambda x: class_names[x])\n",
        "df.sort_values(by=\"class_name\", inplace=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1phYzLOIikxi"
      },
      "source": [
        "pal = sns.cubehelix_palette(10, rot=-.25, light=.7)\n",
        "g = sns.FacetGrid(df, row=\"class_name\", hue=\"class_name\", aspect=10, height=0.8, palette=pal, sharey=False)\n",
        "\n",
        "# Draw the densities in a few steps\n",
        "#g.map(sns.distplot, \"predicted\", kde=False, hist=True, rug=False, bins=100)\n",
        "g.map(sns.kdeplot, \"proba\", clip_on=False, shade=True, alpha=1, lw=3) #, bw=.05)\n",
        "g.map(plt.axhline, y=0, lw=5, clip_on=False)\n",
        "def label(x, color, label):\n",
        "    ax = plt.gca()\n",
        "    ax.text(-0.1, .2, label, fontweight=\"bold\", color=color,\n",
        "            ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
        "\n",
        "g.map(label, \"predicted\")\n",
        "\n",
        "# Set the subplots to overlap\n",
        "g.fig.subplots_adjust(hspace=-.02)\n",
        "g.set_titles(\"\")\n",
        "g.set(yticks=[])\n",
        "g.despine(bottom=True, left=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKWlVhg1kxuT"
      },
      "source": [
        "*There are clearly 2 kinds of classes: the easy ones (Ankle boot, Bag, Sandal, Sneaker, Trouser) and the hard ones, especially Shirt and Pullover.*\n",
        "\n",
        "*We expect to see this in other ways below*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvtM0mSwT-xT"
      },
      "source": [
        "#### Which examples are the hardest to classify?\n",
        "\n",
        "Plot hard example and check whether you'd agree with the model (that they are hard)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MYdoke3eL0P"
      },
      "source": [
        "# Find worst mistakes\n",
        "y_proba_test = model_fc.predict(X_test)\n",
        "mistakes = np.squeeze(np.argwhere(y_pred != y_test))\n",
        "worst_mistakes = np.argsort(-y_proba_test[mistakes,:].max(axis=1))[:n_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1wzwHibg2zO"
      },
      "source": [
        "y_pred[mistakes[:10]], y_test[mistakes[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6WP1AbRgue2"
      },
      "source": [
        "y_pred[worst_mistakes], y_test[worst_mistakes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb71REAgfGFA"
      },
      "source": [
        "plt.figure(figsize=(20, 3))\n",
        "for i, index in enumerate(mistakes[worst_mistakes]):\n",
        "    ax = plt.subplot(1, n_test, i + 1)\n",
        "    title=\"\\n\".join((f\"true: {class_names[y_test[index]]}\", f\"pred: {class_names[y_pred[index]]}\"))\n",
        "    plot_image(X_test[index], ax=ax, title=title);\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO6vmJPFUDl8"
      },
      "source": [
        "#### Which classes are the most difficult to classify? The most confused?\n",
        "\n",
        "This kind of question is best answered by looking at the confusion matrix. If you don't know how to do that, you can look at `sklearn.metrics.confusion_matrix` and `sns.heatmap` for help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eLnpi3Ffph3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred, normalize=\"true\")\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "plt.figure(figsize = (10,8))\n",
        "sns.heatmap(df_cm, annot=True, cmap=\"viridis\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzuK88WhlZtK"
      },
      "source": [
        "*We could almost tell it based on the distribution of probabilities: Pullover and Shirt/T-Shirt are the hardest, then Dress/Coat, the others are pretty easy.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV9gpcHoUpOu"
      },
      "source": [
        "#Visualizing CNN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qv0_btDQXh3"
      },
      "source": [
        "from PIL import Image\n",
        "from functools import partial\n",
        "import os\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-LtaOcf6OjE"
      },
      "source": [
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train_c = (X_train - X_mean) / X_std\n",
        "X_valid_c = (X_valid - X_mean) / X_std\n",
        "X_test_c = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train_c = X_train_c[..., np.newaxis]\n",
        "X_valid_c = X_valid_c[..., np.newaxis]\n",
        "X_test_c = X_test_c[..., np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPnZx8314RKK"
      },
      "source": [
        "\n",
        "DefaultConv2D = partial(keras.layers.Conv2D,\n",
        "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "\n",
        "feature_extractor = keras.models.Sequential([\n",
        "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=128),\n",
        "    DefaultConv2D(filters=128),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    DefaultConv2D(filters=256),\n",
        "    DefaultConv2D(filters=256),\n",
        "    keras.layers.MaxPooling2D(pool_size=2),\n",
        "    keras.layers.Flatten(),\n",
        "])\n",
        "classifier = keras.models.Sequential([\n",
        "    keras.layers.Dense(units=128, activation='selu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=64, activation='selu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(units=10, activation='softmax'),\n",
        "])\n",
        "\n",
        "model = keras.models.Sequential([feature_extractor, classifier])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIxXqRyml8y4"
      },
      "source": [
        "*Let's first look at the embedding produced by _random_ convolution filters, yay!*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zXWt-QfezIt"
      },
      "source": [
        "X_2D = um.fit_transform(feature_extractor(X_vis[:,:,:, np.newaxis]))\n",
        "fig_2D_cnn = plot_interactive_embedding(X_2D, y_vis, X_vis, \n",
        "               \"UMAP embedding of fashion MNIST from feature space fc2, _before_ training\", dscale=0.5)\n",
        "show(fig_2D_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_2ixg89SB_M"
      },
      "source": [
        "*Plot the kernel weights (first layer only, but you get the idea). Before training, it is not very interesting, it's randomly initialized.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaxFdOdPK6YJ"
      },
      "source": [
        "layer_weights = feature_extractor.get_weights()\n",
        "fig, axes = plt.subplots(8, 8)\n",
        "# use global min / max to ensure all weights are shown on the same scale\n",
        "vmin, vmax = layer_weights[0].min(), layer_weights[0].max()\n",
        "\n",
        "for coef, ax in zip(layer_weights[0].T, axes.ravel()):\n",
        "    k1 = (coef.reshape(7, 7) - vmin) / (vmax - vmin)\n",
        "    im = Image.fromarray(k1 * 255)\n",
        "    ax.imshow(im.resize((28,28)))\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORrTnXGD6Xrr"
      },
      "source": [
        "# This is much longer than the feed-forward network so we train only for 10 epochs\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train_c[:10000], y_train[:10000], epochs=10, validation_data=(X_valid_c, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxYkKALnhUNh"
      },
      "source": [
        "score = model.evaluate(X_test_c, y_test)\n",
        "X_new = X_test_c[:10] # pretend we have new images\n",
        "y_pred = model.predict(X_new)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF8dX4thUy7h"
      },
      "source": [
        "You can redo the same visualization as with the feed-forward model. Instead of looking at weights as images, you can plot the CNN's kernel weights as tiny images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9hKeJiH4kyy"
      },
      "source": [
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()\n",
        "sns.despine()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znXDH12esp1r"
      },
      "source": [
        "*There is a risk of overfitting here... this is an example, hyper-parameters have not been tuned.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQTUKS9Z7JM8"
      },
      "source": [
        "X_2D = um.fit_transform(feature_extractor(X_vis[:,:,:, np.newaxis]))\n",
        "fig_2D_cnn = plot_interactive_embedding(X_2D, y_vis, X_vis, \n",
        "               \"UMAP embedding of fashion MNIST from CNN feature space, _after_ training\", dscale=0.5)\n",
        "show(fig_2D_cnn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUpVWEfDdQr6"
      },
      "source": [
        "show(fig_2D_fc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEA9UJlmK4my"
      },
      "source": [
        "layer_weights = feature_extractor.get_weights()\n",
        "fig = plt.figure(figsize=(15, 10))\n",
        "axes = fig.subplots(8, 8)\n",
        "# use global min / max to ensure all weights are shown on the same scale\n",
        "vmin, vmax = layer_weights[0].min(), layer_weights[0].max()\n",
        "\n",
        "for coef, ax in zip(layer_weights[0].T, axes.ravel()):\n",
        "    k1 = (coef.reshape(7, 7) - vmin) / (vmax - vmin)\n",
        "    im = Image.fromarray(k1 * 255)\n",
        "    ax.imshow(im.resize((28,28)))\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C_ONjzwh7C_"
      },
      "source": [
        "*I have seen cleaner filters than this, but still, given that the network is not finely tuned, it's not bad: for instance we can observe some directional filters (vertical, diagnoal)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsrVjo5sVes4"
      },
      "source": [
        "This is the most direct primitive way to watch at what the CNN does. Kernels are not always easy to interpret, especially when they combine to produce the results. Which is why people look at how they affect some test images. One example methods which is history already is DeepDream.\n",
        "\n",
        "If time permits, you can go through the [DeepDream tutorial](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/tutorials/deepdream/deepdream.ipynb) and adapt it here to visualise the layers of your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0jGcy1TShlE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}