{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_ML_and_dataviz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annemariet/tutorials/blob/master/03_ML_and_dataviz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vHNfm17UMeT"
      },
      "source": [
        "# ML4Dataviz / Dataviz4ML, let's practice!\n",
        "\n",
        "This practical is organized in two parts. **The first is \"ML4Dataviz\"**: we are going to explore the MNIST dataset using high-dimensional projections. The goal is that you get a feeling of how complex datasets can be visualized through classical linear and non-linear projections.\n",
        "\n",
        "- You are expected to write mostly the `dataviz` part of the code (ie `matplotlib`, `seaborn`, etc)\n",
        "- You are expected to play with the parameters of the projection functions and explain how they impact performance & display.\n",
        "\n",
        "**The second part is \"Dataviz4ML\"**: We will play with the Fashion-MNIST dataset. A simple feed-forward neural network is implemented. Knowing `keras` is not a prerequisite. The goal in this section is to work you through some examples of how visualization will help you understand & debug your models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dNVmq1rneej"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn import random_projection, decomposition, manifold\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import offsetbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSkOkKfGNXzR"
      },
      "source": [
        "# 1. Visualizing MNIST\n",
        "\n",
        "We use the `scikit-learn` library to download the data and matplotlib for preliminary basic visualizations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKIFi1slt1Km"
      },
      "source": [
        "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True)\n",
        "X = X / 255.\n",
        "y = y.astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAyfq8_4OZyJ"
      },
      "source": [
        "For simplicity we will only work on the first 1000 samples. Not that the usual split uses the first 60k samples for training and the rest for test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbesdVprpwUh"
      },
      "source": [
        "n_vis_samples = 1000\n",
        "w, h = 28, 28\n",
        "X_vis = X[:n_vis_samples]\n",
        "y_vis = y[:n_vis_samples]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZWsx7VgOm8U"
      },
      "source": [
        "It's a good idea to have a look at the raw data to begin with. In the specific case of images we can use a the matplotlib function `imshow` and choose the colormap we prefer. \n",
        "\n",
        "Check the difference between `gray` and `gray_r` colormaps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QfW5FJNrIMP"
      },
      "source": [
        "ax = plt.subplot(111)\n",
        "ax.imshow(X_vis[0,:].reshape(w, h), cmap=plt.cm.gray_r)\n",
        "ax.set_title(y_vis[0])\n",
        "ax.set_axis_off();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ZwNjX8ZudA"
      },
      "source": [
        "We're going to plot images often, so it might help to have a function for this. Define a function plot_image using the following prototype:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkGN_84pZrjA"
      },
      "source": [
        "def plot_image(img, reshape_size=None, cmap=\"binary\", ax=None, title=None):\n",
        "  raise NotImplementedError(\"TODO\")\n",
        "  return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoYpSM-9bRE6"
      },
      "source": [
        "Now you can use this function to plot a bunch of images from the dataset in a grid and have a feeling of what MNIST digits look like. you can play around with the parameters such as colormaps, grid size, etc..\n",
        "\n",
        "You can use the [`plt.subplots`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.subplots.html) from matplotlib to draw several axes on the same figure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUq-OWj9q3eO"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DNORvKqNgMY"
      },
      "source": [
        "## High-dimensional projections to 2D\n",
        "\n",
        "In this exercise we are going to classical methods to visualize MNIST data in 2D.\n",
        "\n",
        "The following sample code is adapted from the [scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vmEe41yrorm"
      },
      "source": [
        "What is the original dimension of MNIST features?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs_UY4mTruKr"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cX-j6v_UqANx"
      },
      "source": [
        "def plot_digits_embedding(X2d, y, title=None, remove_ticks=True):\n",
        "  \"\"\"\n",
        "  Plot a 2D points at positions `X2d` using text labels from `y`.\n",
        "  The data is automatically centered and rescaled to [0,1]\n",
        "  \"\"\"\n",
        "  x_min, x_max = np.min(X2d, 0), np.max(X2d, 0)\n",
        "  X = (X2d - x_min) / (x_max - x_min)\n",
        "\n",
        "  plt.figure(figsize=(20,10))\n",
        "  ax = plt.subplot(111)\n",
        "  for i in range(X.shape[0]):\n",
        "    plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
        "                color=plt.cm.tab10(y[i]),\n",
        "                fontdict={'weight': 'bold', 'size': 9})\n",
        "\n",
        "  if remove_ticks:\n",
        "    plt.xticks([]), plt.yticks([])\n",
        "  if title is not None:\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Lccq4YsukO"
      },
      "source": [
        "We start with a random projection. Even though projecting randomly in 2 dimensions is a rather bad idea, when the original dimension is huge, a random projection to a smaller, still high, dimension might give a nice speedup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCU772sKqj2p"
      },
      "source": [
        "print(\"Computing random projection\")\n",
        "t0 = time()\n",
        "rp = random_projection.SparseRandomProjection(n_components=2, random_state=42)\n",
        "X_projected = rp.fit_transform(X_vis)\n",
        "plot_digits_embedding(X_projected, y_vis, \"Random Projection of the digits (time %.2fs)\" %\n",
        "               (time() - t0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tJIatpFCKZy"
      },
      "source": [
        "### PCA\n",
        "\n",
        "PCA is the go-to method for dimensionality reduction (not only for visualisation). Play around with it to get a feel of how it works."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eu7UmO0MtUPC"
      },
      "source": [
        "You can use scikit-learn's PCA. What is it using under the hood?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sT18tTHEqrCx"
      },
      "source": [
        "print(\"Computing PCA projection\")\n",
        "t0 = time()\n",
        "pca = decomposition.PCA(n_components=2, svd_solver=\"auto\")\n",
        "X_pca = pca.fit_transform(X_vis)\n",
        "plot_digits_embedding(X_pca, y_vis, \n",
        "               \"Principal Components projection of the digits (time %.2fs)\" %\n",
        "               (time() - t0),\n",
        "               remove_ticks=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzENxQGLwrPY"
      },
      "source": [
        " Compare the different `svd_solver` efficiency. `%timeit` is a notebook magic that allows you to compute the time taken by a line of code. Which method is best if you use the full dataset, vs only a few?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W2RpsG0vr53"
      },
      "source": [
        "#TODO %timeit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-YMCnIdNUPf"
      },
      "source": [
        "Since PCA is a linear decomposition, each component is itself an image. Let's have a look at these \"eigenimages\". They are stored in the `components_` attribute of the sklearn `PCA` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ur8rd0uLwcu"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE1VJ7-gyGBQ"
      },
      "source": [
        "Do you think the projection is good? Can you quantify how good it is? How can you explain this? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onxlDgIXOBpD"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8RlvhaWyakG"
      },
      "source": [
        "Can non-linear method give us better results here?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQDbtmOuCFnu"
      },
      "source": [
        "### Multidimensional Scaling (MDS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKvdbaNxvxo3"
      },
      "source": [
        "print(\"Computing MDS embedding\")\n",
        "clf = manifold.MDS(n_components=2, n_init=1, max_iter=100)\n",
        "t0 = time()\n",
        "X_mds = clf.fit_transform(X[:3000])\n",
        "print(\"Done. Stress: %f\" % clf.stress_)\n",
        "plot_digits_embedding(X_mds[:n_vis_samples], y_vis, \n",
        "               \"MDS embedding of the digits (time %.2fs)\" %\n",
        "               (time() - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBPI-3l4BLPn"
      },
      "source": [
        "### t-SNE\n",
        "\n",
        "Play with the various parameters and observe how time/cluster change. [Scikit-learn documentation](https://scikit-learn.org/stable/modules/manifold.html#t-sne) is a good starting point if you want to know more.\n",
        "\n",
        "- perplexity (`perplexity`)\n",
        "- early exaggeration factor (`early_exaggeration`)\n",
        "- learning rate (`learning_rate`)\n",
        "- maximum number of iterations (`n_iter`)\n",
        "- angle (`angle`, not used in the exact method)\n",
        "\n",
        "Which parameters have more impact?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1C-GDVjvAv0"
      },
      "source": [
        "tsne = manifold.TSNE(n_components=2, init='random', random_state=42)\n",
        "t0 = time()\n",
        "X_tsne = tsne.fit_transform(X_vis)\n",
        "\n",
        "plot_digits_embedding(X_tsne, y_vis, \n",
        "               \"t-SNE embedding of the digits (time %.2fs)\" %\n",
        "               (time() - t0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t48HRvfjSLtw"
      },
      "source": [
        "### UMAP\n",
        "\n",
        "UMAP is not yet integrated into scikit-learn but is available as a standalone library that follows the sklearn API. You can have a look at [their doc](https://umap-learn.readthedocs.io/en/latest/parameters.html) to get a better understanding of the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2-CPHOfHfDv"
      },
      "source": [
        "#!pip install umap-learn # run this if you run into an \"ModuleNotFoundError: No module named 'umap'\" error below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em83Z-q7Q0gw"
      },
      "source": [
        "import umap\n",
        "\n",
        "um = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"euclidean\")\n",
        "um.fit(X_vis)\n",
        "X_umap = um.transform(X_vis)\n",
        "\n",
        "plot_digits_embedding(X_umap, y_vis, \n",
        "               \"UMAP embedding of the digits (time %.2fs)\" %\n",
        "               (time() - t0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIu6EulW4mOl"
      },
      "source": [
        "If time permits, you can try combining the methods above: for instance, use PCA to reduce the dimensionality to accelerate either MDS or tSNE. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzLfTNtp44EM"
      },
      "source": [
        "#TODO_OPTIONAL"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwB_56JVQIdq"
      },
      "source": [
        "## The same plots with Bokeh\n",
        "\n",
        "We are going to improve the visualization of `X_projected`, `X_pca`, `X_mds` and `X_tsne` by using `bokeh`.\n",
        "\n",
        "\n",
        "If the Bokeh interactive plots don't show as expected, you might have to revert to a previous version of Bokeh (run `!pip install bokeh==1.3.4`) and restart your runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eECWoWCAvuzS"
      },
      "source": [
        "import bokeh\n",
        "from bokeh.models.sources import ColumnDataSource\n",
        "from bokeh.models.tools import HoverTool\n",
        "import matplotlib as mpl\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "output_notebook()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "priJq0pr7HpB"
      },
      "source": [
        "You are provided with two helper functions to transform images to bokeh-compatible formats. Replace the call to `circle` with `image` and `image_rgba` to change the glyphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f2RBzrOyzL2"
      },
      "source": [
        "def im_colorize_rgba(arr, label):\n",
        "  color = mpl.cm.tab10(label)\n",
        "  if arr.ndim == 1:\n",
        "    out_img = (arr[:, np.newaxis] * np.array(color)).reshape(28,28,4) * 255\n",
        "  elif arr.ndim == 2:\n",
        "    out_img = (arr[:, :, np.newaxis] * np.array(color)) * 255\n",
        "  else:\n",
        "    raise ValueError(\"Image format not handled (yet).\")\n",
        "  out_img = np.flipud(out_img)\n",
        "  return np.squeeze(out_img.astype(np.uint8)).view(np.uint32)\n",
        "\n",
        "def im_reshape_bw(arr):\n",
        "  out_img = arr.reshape(28,28)*255\n",
        "  out_img = np.flipud(out_img)\n",
        "  return out_img\n",
        "  \n",
        "def plot_interactive_embedding(X, y, X_img, title=None):\n",
        "  bokeh_data = ColumnDataSource(\n",
        "          data=dict(\n",
        "              x1 = X[:,0],\n",
        "              x2 = X[:,1],\n",
        "              colors = [\"#%02x%02x%02x\" % (int(r), int(g), int(b)) for r, g, b, _ in \n",
        "                        255*mpl.cm.tab10(y)],\n",
        "              images = [im_reshape_bw(xi) for xi in X_img],\n",
        "              images_rgba = [im_colorize_rgba(xi, yi) for xi, yi in zip(X_img, y)],\n",
        "              labels = [str(yi) for yi in y]\n",
        "          )\n",
        "      )\n",
        "  hover_tsne = HoverTool(tooltips='@labels')\n",
        "  tools_tsne = [hover_tsne, 'pan', 'wheel_zoom', 'reset']\n",
        "  plot_tsne = figure(plot_width=600, plot_height=600, tools=tools_tsne, title=title)\n",
        "\n",
        "  plot_tsne.circle(\"x1\", \"x2\", size=4, fill_color=\"colors\", line_width=0, source=bokeh_data)\n",
        "\n",
        "  return plot_tsne"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRI3BQWxy26p"
      },
      "source": [
        "fig_tsne = plot_interactive_embedding(X_tsne, y_vis, X_vis, \n",
        "               \"t-SNE embedding of the digits\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJbCxY0f2gSi"
      },
      "source": [
        "show(fig_tsne)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws_QR6tvRErV"
      },
      "source": [
        "fig_umap = plot_interactive_embedding(X_umap, y_vis, X_vis, \n",
        "               \"UMAP embedding of the digits\")\n",
        "show(fig_umap)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLJEo7H47xLK"
      },
      "source": [
        "# 2. Visualizing and debugging neural networks\n",
        "\n",
        "In the next section, we're going to reproduce what you're learnt before in your machine learning courses with a focus on the visualizing part of it. You should come out with handy functions that you can reuse later for your own experiments.\n",
        "\n",
        "We will use Fashion-MNIST because that's more fun to visualize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFf-c82uYsjo"
      },
      "source": [
        "Build an image classifier & visualize layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG2jMW1YJiqK"
      },
      "source": [
        "# adapted from https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb\n",
        "try:\n",
        "    # %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBpMnKy9Xf94"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IFDUAn7Xgqe"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPrUnBwAZZxs"
      },
      "source": [
        "X_train_full.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0aiFOZoYruA"
      },
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Eexz4WdcgbM"
      },
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "532vkUdWZel1"
      },
      "source": [
        "plot_image(X_train[0], reshape_size=None, cmap=\"binary\", ax=None, title=class_names[y_train[0]]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNhLg2JgZh64"
      },
      "source": [
        "n_rows = 5\n",
        "n_cols = 20\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        ax = plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plot_image(X_train[index], ax=ax, title=class_names[y_train[index]]);\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq1Fk1i_0EdA"
      },
      "source": [
        "n_vis_samples = 1000\n",
        "X_vis = X_test[:n_vis_samples]\n",
        "y_vis = y_test[:n_vis_samples]\n",
        "X_in = X_vis.reshape(n_vis_samples, 28*28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4l1T9FCz9W3"
      },
      "source": [
        "# Visualize Fashion-MNIST from original input space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6rbrZhrsd0u"
      },
      "source": [
        "## A simple fully connected neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-WSj3hbc-pQ"
      },
      "source": [
        "model_fc = keras.models.Sequential()\n",
        "model_fc.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "model_fc.add(keras.layers.Dense(50, activation=\"relu\", name=\"fc1\"))\n",
        "model_fc.add(keras.layers.Dense(100, activation=\"relu\", name=\"fc2\"))\n",
        "model_fc.add(keras.layers.Dense(10, activation=\"softmax\", name=\"cls_1\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NBxaVsGdGLK"
      },
      "source": [
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UsaqRBQdHnr"
      },
      "source": [
        "keras.utils.plot_model(model_fc, \"my_mnist_model.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg690CczdKcb"
      },
      "source": [
        "model_fc.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrIVOk4D2GdF"
      },
      "source": [
        "# before fitting, visualize the layer outputs\n",
        "\n",
        "# TODO\n",
        "\n",
        "fig_tsne_fc2_rand = plot_interactive_embedding(X_tsne, y_vis, X_vis, \n",
        "               \"t-SNE embedding of fashion MNIST from feature space fc2, without training\")\n",
        "show(fig_tsne_fc2_rand)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dVSnu6gdRcy"
      },
      "source": [
        "# now fit the model\n",
        "history = model_fc.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zyL7RTjC6ZQ"
      },
      "source": [
        "### Looking at the learned weights directly\n",
        "\n",
        "The first layer's weights is a 784xh matrix, so we can observe a few lines of this matrix as images. Can you see patterns emerge?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PICOXzf68fVn"
      },
      "source": [
        "layer_weights = model_fc.get_weights()\n",
        "# TODO explicit layers dimensions & explain what's in this list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FzX2Uw-8dEN"
      },
      "source": [
        "fig, axes = plt.subplots(4, 4)\n",
        "# use global min / max to ensure all weights are shown on the same scale\n",
        "vmin, vmax = layer_weights[0].min(), layer_weights[0].max()\n",
        "\n",
        "for coef, ax in zip(layer_weights[0].T, axes.ravel()):\n",
        "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
        "               vmax=.5 * vmax)\n",
        "    ax.set_xticks(())\n",
        "    ax.set_yticks(())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gor7O4HIDWFJ"
      },
      "source": [
        "### Looking at the latent representations\n",
        "\n",
        "Another way to look at the intermediate layers is through their effect as feature transforms.\n",
        "\n",
        "Plot a view of the points as transformed at different levels in the network. What can you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKiDB7wi3NVz"
      },
      "source": [
        "# after fitting\n",
        "layer_to_plot = [\"fc1\", \"fc2\"]\n",
        "\n",
        "#TODO\n",
        "fig_tsne_fc1 = plot_interactive_embedding(X_tsne, y_vis, X_vis, \n",
        "               \"t-SNE embedding of fashion MNIST from feature space fc1, _after_ training\")\n",
        "\n",
        "# TODO\n",
        "fig_tsne_fc2 = plot_interactive_embedding(X_tsne, y_vis, X_vis, \n",
        "               \"t-SNE embedding of fashion MNIST from feature space fc2, _after_ training\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cfQ0y602gn3"
      },
      "source": [
        "show(fig_tsne_fc1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kugkRKIXriaX"
      },
      "source": [
        "show(fig_tsne_fc2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meXN7VOILgQx"
      },
      "source": [
        "_Answer:_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DdAO9ttLsGf"
      },
      "source": [
        "### Debugging learning\n",
        "\n",
        "Plot the learning curve from the history output of the `fit` call. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DCgKLosdR66"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FCWJNgIL69v"
      },
      "source": [
        "_Answer:_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQMNmIriMLDg"
      },
      "source": [
        "### What do predictions look like?\n",
        "\n",
        "In this part, we're going to look at the output of the classification model itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdWXQdEOdWWd"
      },
      "source": [
        "# Look at a few random predictions\n",
        "n_test = 10\n",
        "X_new = X_test[:n_test]\n",
        "y_proba = model_fc.predict(X_new)\n",
        "y_proba.round(2)\n",
        "y_pred = model_fc.predict_classes(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-SKMdPGdZ8e"
      },
      "source": [
        "#plot images with true & predicted labels on top\n",
        "plt.figure(figsize=(20, 3))\n",
        "#TODO\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De9CpOvpMZqI"
      },
      "source": [
        "#### The distribution of predictions\n",
        "\n",
        "You can use `seaborn`, `bokeh`, or raw `matplotlib` to view these distributions. It might be useful to organise the predictions with dataframes.\n",
        "\n",
        "1. Which classes are predicted the more, the less?\n",
        "2. What is the distribution of the predicted probabilities for each class? Do you see any trend? Does it raise questions?\n",
        "\n",
        "Some tips:\n",
        "- rotate x-axis labels with `plt.xticks(rotation=75)`\n",
        "- move legend outside the plot with `plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)`\n",
        "- change labels on axis ticks with `ax.set_xticklabels(class_names);`\n",
        "- seaborn has both `displot` and `kdeplot`, matplotlib has `hist`...\n",
        "- a [ridge-plot](https://seaborn.pydata.org/examples/kde_ridgeplot.html) could look nice!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmZoTEOitIFQ"
      },
      "source": [
        "# compute test predictions\n",
        "y_proba_test = model_fc.predict(X_test)\n",
        "y_pred = y_proba_test.argmax(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1cLqQTChDof"
      },
      "source": [
        "ax = plt.subplot(111)\n",
        "#TODO question 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHe26uOFvdWs"
      },
      "source": [
        "# ex. df = pd.DataFrame(data=y_proba_test, columns=class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvtM0mSwT-xT"
      },
      "source": [
        "#### Which examples are the hardest to classify?\n",
        "\n",
        "Plot hard example and check whether you'd agree with the model (that they are hard)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MYdoke3eL0P"
      },
      "source": [
        "# Find worst mistakes\n",
        "y_proba_test = model_fc.predict(X_test)\n",
        "mistakes = np.squeeze(np.argwhere(y_pred != y_test))\n",
        "worst_mistakes = np.argsort(-y_proba_test[mistakes,:].max(axis=1))[:n_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1wzwHibg2zO"
      },
      "source": [
        "y_pred[mistakes[:10]], y_test[mistakes[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6WP1AbRgue2"
      },
      "source": [
        "y_pred[worst_mistakes], y_test[worst_mistakes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb71REAgfGFA"
      },
      "source": [
        "plt.figure(figsize=(20, 3))\n",
        "#TODO\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO6vmJPFUDl8"
      },
      "source": [
        "#### Which classes are the most difficult to classify? The most confused?\n",
        "\n",
        "This kind of question is best answered by looking at the confusion matrix. If you don't know how to do that, you can look at `sklearn.metrics.confusion_matrix` and `sns.heatmap` for help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eLnpi3Ffph3"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV9gpcHoUpOu"
      },
      "source": [
        "#Visualizing CNN models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-LtaOcf6OjE"
      },
      "source": [
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train_c = (X_train - X_mean) / X_std\n",
        "X_valid_c = (X_valid - X_mean) / X_std\n",
        "X_test_c = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train_c = X_train_c[..., np.newaxis]\n",
        "X_valid_c = X_valid_c[..., np.newaxis]\n",
        "X_test_c = X_test_c[..., np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPnZx8314RKK"
      },
      "source": [
        "from functools import partial\n",
        "import os\n",
        "\n",
        "convmodel_file = \"convnet.h5\"\n",
        "\n",
        "if os.path.exists(convmodel_file):\n",
        "  model = keras.models.load_model(convmodel_file)\n",
        "  trained = True\n",
        "else:\n",
        "  DefaultConv2D = partial(keras.layers.Conv2D,\n",
        "                          kernel_size=3, activation='relu', padding=\"SAME\")\n",
        "\n",
        "  feature_extractor = keras.models.Sequential([\n",
        "      DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
        "      keras.layers.MaxPooling2D(pool_size=2),\n",
        "      DefaultConv2D(filters=128),\n",
        "      DefaultConv2D(filters=128),\n",
        "      keras.layers.MaxPooling2D(pool_size=2),\n",
        "      DefaultConv2D(filters=256),\n",
        "      DefaultConv2D(filters=256),\n",
        "      keras.layers.MaxPooling2D(pool_size=2),\n",
        "      keras.layers.Flatten(),\n",
        "  ])\n",
        "  classifier = keras.models.Sequential([\n",
        "      keras.layers.Dense(units=128, activation='selu'),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(units=64, activation='selu'),\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.Dense(units=10, activation='softmax'),\n",
        "  ])\n",
        "\n",
        "  model = keras.models.Sequential([feature_extractor, classifier])\n",
        "  trained = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORrTnXGD6Xrr"
      },
      "source": [
        "if not trained:\n",
        "  model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "  history = model.fit(X_train_c[:10000], y_train[:10000], epochs=10, validation_data=(X_valid_c, y_valid))\n",
        "  model.save(convmodel_file)\n",
        "score = model.evaluate(X_test_c, y_test)\n",
        "X_new = X_test_c[:10] # pretend we have new images\n",
        "y_pred = model.predict(X_new)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF8dX4thUy7h"
      },
      "source": [
        "You can redo the same visualization as with the feed-forward model. Instead of looking at weights as images, you can plot the CNN's kernel weights as tiny images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9hKeJiH4kyy"
      },
      "source": [
        "# plot history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQTUKS9Z7JM8"
      },
      "source": [
        "# plot tSNE or UMAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsrVjo5sVes4"
      },
      "source": [
        "If time permits, you can go through the [DeepDream tutorial](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/tutorials/deepdream/deepdream.ipynb) and adapt it here to visualise the layers of your network."
      ]
    }
  ]
}